name: Benchmark

on:
  pull_request:
    paths:
      - '**.go'
      - 'go.mod'
      - 'go.sum'
      - '.github/workflows/benchmark.yaml'
  push:
    branches: [main]
    paths:
      - '**.go'
      - 'go.mod'
      - 'go.sum'
      - '.github/workflows/benchmark.yaml'
  workflow_dispatch:

permissions:
  contents: write
  pull-requests: write

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version-file: 'go.mod'
          cache: true

      - name: Install dependencies
        run: |
          go mod download
          go install -mod=mod github.com/sivchari/govalid/cmd/govalid

      - name: Install yq for YAML parsing and benchstat
        run: |
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          go install golang.org/x/perf/cmd/benchstat@latest

      - name: Generate test code
        run: |
          cd test
          go generate

      - name: Run benchmarks
        run: |
          cd test/benchmark
          
          # Use shorter benchmark time for CI (3s instead of 10s)
          BENCHMARK_TIME="3s"
          COUNT="5"
          TIMEOUT="30m"
          
          # Run benchmarks and separate by library
          echo "Running govalid benchmarks..."
          go test -bench=BenchmarkGoValid -benchmem -benchtime=${BENCHMARK_TIME} -count=${COUNT} -timeout=${TIMEOUT} > ../../govalid-results.txt
          
          echo "Running go-playground benchmarks..."
          go test -bench=BenchmarkGoPlayground -benchmem -benchtime=${BENCHMARK_TIME} -count=${COUNT} -timeout=${TIMEOUT} > ../../playground-results.txt
          
          # Combine all results for raw output
          cat ../../govalid-results.txt ../../playground-results.txt > ../../benchmark-results.txt

      - name: Parse benchmark results with benchstat
        id: parse
        run: |
          # Create a summary of benchmark results
          echo "## Benchmark Results" > benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "**Platform:** $(uname -s) $(uname -m)" >> benchmark-summary.md
          echo "**Go version:** $(go version | awk '{print $3}')" >> benchmark-summary.md
          echo "**Date:** $(date +%Y-%m-%d)" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          
          # Add metrics explanation section
          echo "### ðŸ“Š Benchmark Metrics Explained" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "Our benchmarks measure three key performance indicators:" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "- **ns/op (nanoseconds per operation)**: Execution time - *lower is better*" >> benchmark-summary.md
          echo "- **B/op (bytes per operation)**: Memory allocated - *lower is better*" >> benchmark-summary.md  
          echo "- **allocs/op (allocations per operation)**: Number of memory allocations - *lower is better*" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "**Why this matters:** Faster execution with fewer allocations means better performance and lower memory pressure in production applications." >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          
          # Use benchstat for statistical comparison
          echo "### ðŸš€ Performance Comparison vs go-playground/validator" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "*Statistical analysis using benchstat for reliable performance comparison*" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          
          # Process comparison validators from config
          yq e '.comparison_validators[] | [.name, .description, .govalid_benchmark, .competitor_benchmark] | @csv' benchmark.yaml | while IFS=',' read -r name description govalid_bench competitor_bench; do
            # Remove quotes from CSV output
            name=$(echo "$name" | sed 's/"//g')
            description=$(echo "$description" | sed 's/"//g')
            govalid_bench=$(echo "$govalid_bench" | sed 's/"//g')
            competitor_bench=$(echo "$competitor_bench" | sed 's/"//g')
            
            # Extract specific benchmark results for benchstat comparison
            grep "^$govalid_bench-" govalid-results.txt > govalid-${name,,}.txt 2>/dev/null || true
            grep "^$competitor_bench-" playground-results.txt > playground-${name,,}.txt 2>/dev/null || true
            
            if [ -s "govalid-${name,,}.txt" ] && [ -s "playground-${name,,}.txt" ]; then
              echo "#### $name - $description" >> benchmark-summary.md
              echo "" >> benchmark-summary.md
              echo '```' >> benchmark-summary.md
              benchstat "playground-${name,,}.txt" "govalid-${name,,}.txt" >> benchmark-summary.md 2>/dev/null || echo "Benchstat comparison failed for $name" >> benchmark-summary.md
              echo '```' >> benchmark-summary.md
              echo "" >> benchmark-summary.md
            fi
          done
          
          # Add section for govalid-specific markers
          echo "" >> benchmark-summary.md
          echo "### ðŸŽ¯ govalid-Specific Validators" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "*These validators are unique to govalid and not available in go-playground/validator*" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          
          # Process govalid-specific validators from config
          yq e '.govalid_specific[] | [.name, .description, .benchmark] | @csv' benchmark.yaml | while IFS=',' read -r name description benchmark; do
            # Remove quotes from CSV output
            name=$(echo "$name" | sed 's/"//g')
            description=$(echo "$description" | sed 's/"//g')
            benchmark=$(echo "$benchmark" | sed 's/"//g')
            
            # Extract specific benchmark results for benchstat
            grep "^$benchmark-" govalid-results.txt > govalid-specific-${name,,}.txt 2>/dev/null || true
            
            if [ -s "govalid-specific-${name,,}.txt" ]; then
              echo "#### $name - $description" >> benchmark-summary.md
              echo "" >> benchmark-summary.md
              echo '```' >> benchmark-summary.md
              benchstat "govalid-specific-${name,,}.txt" >> benchmark-summary.md 2>/dev/null || echo "Benchstat analysis failed for $name" >> benchmark-summary.md
              echo '```' >> benchmark-summary.md
              echo "" >> benchmark-summary.md
            fi
          done
          
          # Add performance insights section
          echo "" >> benchmark-summary.md
          echo "### ðŸ’¡ Performance Insights" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "**Key Advantages of govalid:**" >> benchmark-summary.md
          echo "- **Zero Allocations**: Most validators achieve 0 B/op and 0 allocs/op" >> benchmark-summary.md
          echo "- **Compile-time Generation**: Code is generated at build time, eliminating runtime reflection" >> benchmark-summary.md
          echo "- **Optimal Performance**: Nanosecond-level execution times for most validations" >> benchmark-summary.md
          echo "- **Memory Efficiency**: No heap allocations during validation in most cases" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo "**When to use govalid vs alternatives:**" >> benchmark-summary.md
          echo "- Choose govalid for high-performance applications where validation speed matters" >> benchmark-summary.md
          echo "- Ideal for microservices, APIs, and data processing pipelines" >> benchmark-summary.md
          echo "- Perfect for applications with strict memory allocation budgets" >> benchmark-summary.md
          
          # Save raw results
          echo "" >> benchmark-summary.md
          echo "<details>" >> benchmark-summary.md
          echo "<summary>Raw Benchmark Results</summary>" >> benchmark-summary.md
          echo "" >> benchmark-summary.md
          echo '```' >> benchmark-summary.md
          cat benchmark-results.txt >> benchmark-summary.md
          echo '```' >> benchmark-summary.md
          echo "</details>" >> benchmark-summary.md
          
          # Set output for PR comment
          {
            echo 'summary<<EOF'
            cat benchmark-summary.md
            echo EOF
          } >> $GITHUB_OUTPUT

      - name: Comment PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const summary = process.env.BENCHMARK_SUMMARY;
            
            // Find existing comments
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });
            
            // Find all bot comments related to benchmarks
            const benchmarkComments = comments.filter(comment => 
              comment.user.type === 'Bot' && 
              comment.body.includes('## Benchmark Results')
            );
            
            // Delete old benchmark comments (keep only the most recent one)
            const latestBenchmarkComment = benchmarkComments
              .sort((a, b) => new Date(b.created_at) - new Date(a.created_at))[0];
            
            // Delete old comments
            for (const comment of benchmarkComments) {
              if (comment.id !== latestBenchmarkComment?.id) {
                try {
                  await github.rest.issues.deleteComment({
                    owner: context.repo.owner,
                    repo: context.repo.repo,
                    comment_id: comment.id
                  });
                  console.log(`Deleted old comment ${comment.id}`);
                } catch (error) {
                  console.log(`Failed to delete comment ${comment.id}: ${error.message}`);
                }
              }
            }
            
            const body = `${summary}\n\n---\n*Automated benchmark report generated by GitHub Actions*`;
            
            if (latestBenchmarkComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: latestBenchmarkComment.id,
                body: body
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }
        env:
          BENCHMARK_SUMMARY: ${{ steps.parse.outputs.summary }}

      - name: Update README on main
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          # Extract benchmark date and platform info
          BENCH_DATE=$(date +%Y-%m-%d)
          PLATFORM="$(uname -s) $(uname -r) $(uname -m)"
          GO_VERSION=$(go version | awk '{print $3}')
          
          # Create updated README content for test/benchmark with benchstat format
          cat > test/benchmark/README.md << EOF
          # Benchmark Results
          
          This document contains comprehensive performance comparison results between govalid and go-playground/validator.
          
          ## Understanding the Metrics
          
          Our benchmarks measure three key performance indicators:
          
          - **ns/op (nanoseconds per operation)**: Execution time - *lower is better*
          - **B/op (bytes per operation)**: Memory allocated - *lower is better*  
          - **allocs/op (allocations per operation)**: Number of memory allocations - *lower is better*
          
          **Why this matters:** Faster execution with fewer allocations means better performance and lower memory pressure in production applications.
          
          ## Latest Results
          
          **Benchmarked on:** ${BENCH_DATE}
          **Platform:** ${PLATFORM}
          **Go version:** ${GO_VERSION}
          
          *Statistical analysis using benchstat for reliable performance comparison*
          
          ## Performance Comparison vs go-playground/validator
          
          EOF
          
          # Generate benchstat comparisons for documentation
          yq e '.comparison_validators[] | [.name, .description, .govalid_benchmark, .competitor_benchmark] | @csv' benchmark.yaml | while IFS=',' read -r name description govalid_bench competitor_bench; do
            # Remove quotes from CSV output
            name=$(echo "$name" | sed 's/"//g')
            description=$(echo "$description" | sed 's/"//g')
            govalid_bench=$(echo "$govalid_bench" | sed 's/"//g')
            competitor_bench=$(echo "$competitor_bench" | sed 's/"//g')
            
            # Extract specific benchmark results for benchstat comparison
            grep "^$govalid_bench-" govalid-results.txt > govalid-${name,,}.txt 2>/dev/null || true
            grep "^$competitor_bench-" playground-results.txt > playground-${name,,}.txt 2>/dev/null || true
            
            if [ -s "govalid-${name,,}.txt" ] && [ -s "playground-${name,,}.txt" ]; then
              echo "### $name - $description" >> test/benchmark/README.md
              echo "" >> test/benchmark/README.md
              echo '```' >> test/benchmark/README.md
              benchstat "playground-${name,,}.txt" "govalid-${name,,}.txt" >> test/benchmark/README.md 2>/dev/null || echo "Benchstat comparison failed for $name" >> test/benchmark/README.md
              echo '```' >> test/benchmark/README.md
              echo "" >> test/benchmark/README.md
            fi
          done
          
          # Add govalid-specific validators section
          echo "## govalid-Specific Validators" >> test/benchmark/README.md
          echo "" >> test/benchmark/README.md
          echo "*These validators are unique to govalid and not available in go-playground/validator*" >> test/benchmark/README.md
          echo "" >> test/benchmark/README.md
          
          yq e '.govalid_specific[] | [.name, .description, .benchmark] | @csv' benchmark.yaml | while IFS=',' read -r name description benchmark; do
            # Remove quotes from CSV output
            name=$(echo "$name" | sed 's/"//g')
            description=$(echo "$description" | sed 's/"//g')
            benchmark=$(echo "$benchmark" | sed 's/"//g')
            
            # Extract specific benchmark results for benchstat
            grep "^$benchmark-" govalid-results.txt > govalid-specific-${name,,}.txt 2>/dev/null || true
            
            if [ -s "govalid-specific-${name,,}.txt" ]; then
              echo "### $name - $description" >> test/benchmark/README.md
              echo "" >> test/benchmark/README.md
              echo '```' >> test/benchmark/README.md
              benchstat "govalid-specific-${name,,}.txt" >> test/benchmark/README.md 2>/dev/null || echo "Benchstat analysis failed for $name" >> test/benchmark/README.md
              echo '```' >> test/benchmark/README.md
              echo "" >> test/benchmark/README.md
            fi
          done
          
          # Add raw benchmark data section
          echo "## Raw Benchmark Data" >> test/benchmark/README.md
          echo "" >> test/benchmark/README.md
          echo "\`\`\`" >> test/benchmark/README.md
          cat benchmark-results.txt >> test/benchmark/README.md
          echo "\`\`\`" >> test/benchmark/README.md
          echo "" >> test/benchmark/README.md
          
          # Add performance summary
          echo "## Performance Summary" >> test/benchmark/README.md
          echo "" >> test/benchmark/README.md
          echo "**Key Advantages of govalid:**" >> test/benchmark/README.md
          echo "- **Zero Allocations**: Most validators achieve 0 B/op and 0 allocs/op" >> test/benchmark/README.md
          echo "- **Compile-time Generation**: Code is generated at build time, eliminating runtime reflection" >> test/benchmark/README.md
          echo "- **Optimal Performance**: Nanosecond-level execution times for most validations" >> test/benchmark/README.md
          echo "- **Memory Efficiency**: No heap allocations during validation in most cases" >> test/benchmark/README.md
          echo "- **Statistical Reliability**: Results verified using benchstat for accurate performance measurement" >> test/benchmark/README.md
          echo "" >> test/benchmark/README.md
          echo "**When to use govalid vs alternatives:**" >> test/benchmark/README.md
          echo "- Choose govalid for high-performance applications where validation speed matters" >> test/benchmark/README.md
          echo "- Ideal for microservices, APIs, and data processing pipelines" >> test/benchmark/README.md
          echo "- Perfect for applications with strict memory allocation budgets" >> test/benchmark/README.md
          
          # Run sync-benchmarks to update docs
          if [ -f scripts/sync-benchmarks.sh ]; then
            ./scripts/sync-benchmarks.sh
          fi
          
          # Configure git
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          
          # Check if there are changes
          if git diff --quiet; then
            echo "No changes to benchmark results"
          else
            git add test/benchmark/README.md
            git add docs/content/benchmarks.md docs/content/_index.md 2>/dev/null || true
            git commit -m "Update benchmark results
          
          Automated update from GitHub Actions
          Platform: ${PLATFORM}
          Date: ${BENCH_DATE}"
            git push
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.txt
            benchmark-summary.md
          retention-days: 30

